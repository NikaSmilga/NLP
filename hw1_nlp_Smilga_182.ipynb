{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from pprint import pprint\n",
    "import sqlite3\n",
    "from html import unescape\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я буду доставать отзывы на фильм Ларса фон Триера \"Антихрист\" с Кинопоиска. Кинопоиск позволяет открыть 200 отзывов сразу, поэтому нам не придется мучиться с доставанием отзывов из разных мест. С помощью Супа распарсю полученный html-файл, достав сначала все хорошие, а затем все плохие отзывы. Положу 35 первых хороших и 35 первых плохих в соответствующие списки. Также сразу сделаю тестовые выборки -- плохих и хороших."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all(url_one):\n",
    "    session = requests.session()\n",
    "    ua = UserAgent(verify_ssl=False)\n",
    "    req = session.get(url_one, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    good_reviews = soup.findAll('div', {'class': \"response good\"})\n",
    "    goodie = []\n",
    "    for i in good_reviews:\n",
    "        i_1 = i.find('div', {'class': \"brand_words\"}).text.replace('\\n', '').replace('\\xa0', ' ').replace('\\r', '')\n",
    "        goodie.append(i_1)\n",
    "    goodies = goodie[:35]\n",
    "    g_test = goodie[35:40]\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    bad_reviews = soup.findAll('div', {'class': \"response bad\"})\n",
    "    badie = []\n",
    "    for i in bad_reviews:\n",
    "        i_1 = i.find('div', {'class': \"brand_words\"}).text.replace('\\n', '').replace('\\xa0', ' ').replace('\\r', '')\n",
    "        badie.append(i_1)\n",
    "    badies = badie[:35]\n",
    "    b_test = badie[35:40]\n",
    "    return(goodies, badies, g_test, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью НЛТК и ПайМорфи токенизирую текст и приведу все слова к начальным формам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(goodies):\n",
    "    prepr = []\n",
    "    for i in goodies:\n",
    "        text_py1 = []\n",
    "        morph = MorphAnalyzer()\n",
    "        text_py = [w.lower() for w in word_tokenize(i) if w.isalpha()]\n",
    "        text_len = len(text_py)\n",
    "        review = []\n",
    "        for word in text_py:\n",
    "            new_word = morph.parse(word)\n",
    "            new_word1 = new_word[0]\n",
    "            review.append(new_word1.normal_form)\n",
    "        review = ' '.join(review)\n",
    "        prepr.append(review)\n",
    "    return prepr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам множества слов, которые есть только в хороших (а затем только в плохих) отзывах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique(first, second):\n",
    "    true = []\n",
    "    all_f = ' '.join(first).split()\n",
    "    all_s = ' '.join(second).split()\n",
    "    for i in all_f:\n",
    "        if i not in all_s:\n",
    "            true.append(i)\n",
    "    return true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаю частоту встречаемости каждого из слов в отзывах, уберу те слова, которые встречаются по одному разу. _(Я написала эту функцию, тк она была в задании, но с ней моя accuracy падает, видимо из-за маленькой выборки и красноречия авторов отзывов, поэтому я не буду ее использовать далее в самой программе. Но она рабочая и оставлена здесь, чтобы мне не снизили балл за невыполнение пункта)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_commons(t):\n",
    "    c = collections.Counter(t)\n",
    "    commons = [word for word,v in c.items() if v > 1]\n",
    "    return commons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определю, хороший отзыв или плохой, посчитав, сколько в нем слов из хорошего и плохого набора. Если слов из хорошего набора больше -- считаем отзыв хорошим, если меньше -- плохим. Если равное количество -- относим его к противоречивым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_or_bad(r, t_g, t_b):\n",
    "    bads = 0\n",
    "    goods = 0\n",
    "    morph = MorphAnalyzer()\n",
    "    text_py = [w.lower() for w in word_tokenize(r) if w.isalpha()]\n",
    "    text_len = len(text_py)\n",
    "    rev = []\n",
    "    for word in text_py:\n",
    "        new_word = morph.parse(word)\n",
    "        new_word1 = new_word[0]\n",
    "        rev.append(new_word1.normal_form)\n",
    "    for i in rev:\n",
    "        if i in t_b:\n",
    "            bads += 1\n",
    "        elif i in t_g:\n",
    "            goods += 1\n",
    "    if bads > goods:\n",
    "        ton = 'bad'\n",
    "    elif bads < goods:\n",
    "        ton = 'good'\n",
    "    else:\n",
    "        ton = 'controversial'\n",
    "    return ton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самостоятельно посчитаю accuracy своей функции, разделив количество правильных ответов на общее количество отзывов в тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(test_g, test_b, g_un, b_un):\n",
    "    acc = 0\n",
    "    for review in test_g:\n",
    "        if good_or_bad(review, g_un, b_un) == 'good':\n",
    "            acc += 1\n",
    "    for review in test_b:\n",
    "        if good_or_bad(review, g_un, b_un) == 'bad':\n",
    "            acc += 1\n",
    "    acc_tot = acc/(len(test_g)+len(test_b))\n",
    "    return acc_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь запущу все полученные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goodies1, badies1, g_test1, b_test1 = get_all('https://www.kinopoisk.ru/film/408909/reviews/ord/date/status/all/perpage/200/')\n",
    "goodies2, badies2, g_test2, b_test2 = get_all('https://www.kinopoisk.ru/film/408909/reviews/ord/date/status/all/perpage/200/page/2/')\n",
    "goodies = goodies1 + goodies2\n",
    "badies = badies1 + badies2\n",
    "g_test = g_test1 + g_test2\n",
    "b_test = b_test1 + b_test2\n",
    "g_prep = prep(goodies)\n",
    "b_prep = prep(badies)\n",
    "g_un = get_unique(g_prep, b_prep)\n",
    "b_un = get_unique(b_prep, g_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:', get_acc(g_test, b_test, g_un, b_un))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы увеличить accuracy, можно:\n",
    "1. Взять бóльшую выборку отзывов, добавив другие фильмы того же режиссера (они схожей тематики, так что ключевые слова не будут сильно различаться)\n",
    "2. Можно было бы воспользоваться word2vec, чтобы смотреть не только сами слова, встретившиеся в негативных/позитивных отзывах, но и синонимичные, близкие к ним по векторному представлению\n",
    "3. Посмотреть не слова, а н-граммы \n",
    "4. Найти какую-то черту, отличающую положительные отзывы на этом сайте от отрицательных и смотреть по ней (например, возможно, авторы отрицательных отзывов более эмоциональны в своем гневе, тогда можно будет принимать во внимание знаки препинания)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
