{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В мой текст на русском языке я включила почти все сложные случаи, описанные в статье О.Н. Ляшевской, в частности: омоформы (ели), общепринятые сокращения (тыс., м.), аббревиатуры (МГУ, МФТИ), фамилии на -ий/-ов (их можно спутать с прилагательными), авторские неологизмы (потеннисить) и сложные слова, разделенные дефисом. Все эти слова неоднозачны в трактовке части речи и считаются непростыми для частеречных таггеров: для работы с ними необходимо обращаться к контексту либо внутреннему устройству слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = 'Около двух тыс. студентов и выпускников МГУ и МФТИ, среди них такие известные ученые, как Антон Иванович Петров, Леонид Иосифович Певчих и Алексей Ефимович Лидский, примут участие в соревнованиях по лаун-теннису. Некоторые выпускники-ученые приедут прямиком из Нью-Йорка, Абу-Даби и других городов и стран, чтобы потеннисить на славу. Данные участники должны будут выслать свои персональные данные заранее по электронной почте. Теннисный корт, длина которого достигает десяти м., а ширина – двадцати пяти м., будут окружать специально посаженные ели и березы, которые, как предполагается, поднимут боевой дух участников, напомнив им о родине-матушке. Об этом событии напишут в различных газетах и журналах: выйдут тексты по-русски, по-французски и по-немецки.'\n",
    "exclude = [',','.', '!', '?', ':', '–', '(', ')']\n",
    "exclude = set(exclude)\n",
    "text = ''.join(ch for ch in text if ch not in exclude)\n",
    "text_pos = '''Около ADP\n",
    "двух NUM\n",
    "тыс NUM\n",
    "студентов NOUN\n",
    "и CONJ\n",
    "выпускников NOUN\n",
    "МГУ NOUN\n",
    "и CONJ\n",
    "МФТИ NOUN\n",
    "среди ADP\n",
    "них PRON\n",
    "такие DET\n",
    "известные ADJ\n",
    "ученые NOUN\n",
    "как CONJ\n",
    "Антон PROPN\n",
    "Иванович PROPN\n",
    "Петров PROPN\n",
    "Леонид PROPN\n",
    "Иосифович PROPN\n",
    "Певчих PROPN\n",
    "и CONJ\n",
    "Алексей PROPN\n",
    "Ефимович PROPN\n",
    "Лидский PROPN\n",
    "примут VERB\n",
    "участие NOUN\n",
    "в ADP\n",
    "соревнованиях NOUN\n",
    "по ADP\n",
    "лаун-теннису NOUN\n",
    "Некоторые PROPN\n",
    "выпускники-ученые NOUN\n",
    "приедут VERB\n",
    "прямиком ADV\n",
    "из ADP\n",
    "Нью-Йорка PROPN\n",
    "Абу-Даби PROPN\n",
    "и CONJ\n",
    "других PROPN\n",
    "городов NOUN\n",
    "и CONJ\n",
    "стран NOUN\n",
    "чтобы CONJ\n",
    "потеннисить VERB\n",
    "на ADP\n",
    "славу NOUN\n",
    "Данные ADJ\n",
    "участники NOUN\n",
    "должны ADJ\n",
    "будут AUX\n",
    "выслать VERB\n",
    "свои DET\n",
    "персональные ADJ\n",
    "данные NOUN\n",
    "заранее ADV\n",
    "по ADP\n",
    "электронной ADJ\n",
    "почте NOUN\n",
    "Теннисный ADJ\n",
    "корт NOUN\n",
    "длина NOUN\n",
    "которого PRON\n",
    "достигает VERB\n",
    "десяти NUM\n",
    "м NOUN\n",
    "а CONJ\n",
    "ширина NOUN\n",
    "двадцати NUM\n",
    "пяти NUM\n",
    "м NOUN\n",
    "будут AUX\n",
    "окружат VERB\n",
    "специально ADV\n",
    "посаженные VERB\n",
    "ели NOUN\n",
    "и CONJ\n",
    "березы NOUN\n",
    "которые PROPN\n",
    "как CONJ\n",
    "предполагается VERB\n",
    "поднимут VERB\n",
    "боевой ADJ\n",
    "дух NOUN\n",
    "участников NOUN\n",
    "напомнив VERB\n",
    "им PRON\n",
    "о ADP\n",
    "родине-матушке NOUN\n",
    "Об ADP\n",
    "этом DET\n",
    "событии NOUN\n",
    "напишут VERB\n",
    "в ADP\n",
    "различных ADJ\n",
    "газетах NOUN\n",
    "и CONJ\n",
    "журналах NOUN\n",
    "выйдут VERB\n",
    "тексты NOUN\n",
    "по-русски ADV\n",
    "по-французски ADV\n",
    "и CONJ\n",
    "по-немецки ADV''' \n",
    "pos = []\n",
    "for i in text_pos.split('\\n'):\n",
    "    pos.append(i.split()[1])\n",
    "gold = pos.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти две функции приводят теги трех использовавшихся парсеров к моему набору золотого стандарта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_to_udp(pos, type_p):\n",
    "    for i in range(len(pos)):\n",
    "        if type_p == 'natasha':\n",
    "            if pos[i] == 'AUX':\n",
    "                pos[i] = 'VERB'\n",
    "            if pos[i] == 'PROPN':\n",
    "                pos[i] = 'NOUN'\n",
    "            if pos[i] == 'NPRO' or pos[i] == 'DET':\n",
    "                pos[i] = 'PRON'\n",
    "            if pos[i] == 'SCONJ' or pos[i] == 'CCONJ':\n",
    "                pos[i] = 'CONJ'\n",
    "        if type_p == 'mystem':\n",
    "            if pos[i] == 'V':\n",
    "                pos[i] = 'VERB'\n",
    "            if pos[i] == 'S':\n",
    "                pos[i] = 'NOUN'\n",
    "            if pos[i] == 'A':\n",
    "                pos[i] = 'ADJ'\n",
    "            if pos[i] == 'ADVPRO':\n",
    "                pos[i] = 'ADV'\n",
    "            if pos[i] == 'ADVPRO':\n",
    "                pos[i] = 'ADV'\n",
    "            if pos[i] == 'APRO' or pos[i] == 'SPRO':\n",
    "                pos[i] = 'PRON'\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def py_to_udp(pos, type_p):\n",
    "    pymorphy = []\n",
    "    for i in range(len(pos)):\n",
    "        if pos[i] == 'ADJF' or pos[i] =='ADJS':\n",
    "            pymorphy.append('ADJ')\n",
    "        elif pos[i] == 'ADVB':\n",
    "            pymorphy.append('ADV')\n",
    "        elif pos[i] == 'NUMR':\n",
    "            pymorphy.append('NUM')\n",
    "        elif pos[i] == 'INFN' or pos[i] == 'GRND' or pos[i] == 'PRTF':\n",
    "            pymorphy.append('VERB')\n",
    "        elif pos[i] == 'PREP':\n",
    "            pymorphy.append('ADP')\n",
    "        elif pos[i] == 'NPRO':\n",
    "            pymorphy.append('PRON')\n",
    "        else:\n",
    "            pymorphy.append(str(pos[i]))\n",
    "    return pymorphy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализируем текст с помощью пайморфи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "import re\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "pm2 = []\n",
    "for word in text.split():\n",
    "    p = morph.parse(word)[0]\n",
    "    pm2.append(p.tag.POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализируем текст с помощью майстема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "mystem = []\n",
    "for word in text.split():\n",
    "    an = m.analyze(word)[0].get('analysis')[0].get('gr')\n",
    "    pos = re.split(r'=|\\,', an)[0]\n",
    "    mystem.append(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализируем текст с помощью Наташи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (MorphVocab, NewsMorphTagger, NewsEmbedding, Doc, Segmenter)\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "segmenter = Segmenter()\n",
    "doc = Doc(text)\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "natasha = [_.pos for _ in doc.tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем функцию для приведения тегов к единому стандарту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "natasha_new = other_to_udp(natasha, 'natasha')\n",
    "pm_new = py_to_udp(pm2, 'py')\n",
    "mystem_new = other_to_udp(mystem, 'mystem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция считает accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pos, gold):\n",
    "    count = 0\n",
    "    for i in range(len(pos)):\n",
    "        if pos[i] == gold[i]:\n",
    "            count+=1\n",
    "    acc = count/len(pos)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательно считаем accuracy каждого парсера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natasha's accuracy: 0.8076923076923077\n",
      "mystem's accuracy: 0.6634615384615384\n",
      "pymorphy's accuracy: 0.7692307692307693\n"
     ]
    }
   ],
   "source": [
    "print(\"natasha's accuracy:\", get_acc(natasha_new, gold))\n",
    "print(\"mystem's accuracy:\", get_acc(mystem_new, gold))\n",
    "print(\"pymorphy's accuracy:\", get_acc(pm_new, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция выделяет синтаксические группы. Я выбрала группы прилагательное+существительное (поскольку отзывы чаще всего описательны, в них много экспрессивных адъективных конструкций, выражающих отношение автора к фильму), не+глагол (я заметила, что в отзывах на кинопоиске люди часто высказывают свое мнение в формате \"не зашло\", \"не может никому понравиться\" и так далее, поэтому такие конструкции предположительно будут маркерами отрицательных отзывов) и наречие+глагол/глагол+наречие (чтобы выделять описательные, но глагольные конструкции типа \"потрясающе снято\", \"сделано качественно\" и тд). Я использовала пайморфи, хотя Наташа работает лучше, тк с Наташей сложнее работать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups(text):\n",
    "    adj_noun = []\n",
    "    no = []\n",
    "    adv_p = []\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    i = re.split(r'\\?|!|\\.', text)\n",
    "    for line in i:\n",
    "        token = nltk.word_tokenize(line)\n",
    "        bigram = list(ngrams(token, 2)) \n",
    "        for i in bigram:\n",
    "            if morph.parse(i[0])[0].tag.POS == 'ADJF' and morph.parse(i[1])[0].tag.POS == 'NOUN':\n",
    "                adj_noun.append(i)\n",
    "            if i[0] == 'не' and morph.parse(i[1])[0].tag.POS == 'VERB':\n",
    "                no.append(i)\n",
    "            if morph.parse(i[0])[0].tag.POS == 'ADVB' and morph.parse(i[1])[0].tag.POS == 'VERB' or morph.parse(i[0])[0].tag.POS == 'VERB' and morph.parse(i[1])[0].tag.POS == 'ADVB':\n",
    "                adv_p.append(i)\n",
    "    return adj_noun, no, adv_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имплементацию этого кода в старую домашку см. в соседней тетрадке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тексте на английском языке я использовала в основном омоформы, то есть слова,  которые могут принадлежать к разным частям речи и так же как и в русском требуют контекстадля работыс ними. В их числе были глаголы-существительные (protest), местоимения-союзы (whatever), предлоги-частицы (to), существительные-прилагательные (right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = '''I am going to protest for a right to name this place whatever I want. You need to rate every rain you had in your life. Give me a sign if you want to smoke some stuff with me. She is a real tease. Use some time to vote! Wake up and help me find my watch! He gave me a wave and a wink. His voice is so calm it makes me yawn. I heard his voice and looked upstage. I want to take a risk and participate in the race for a gold flower. Did you burn the cake?'''\n",
    "exclude = [',','.', '!', '?', ':', '–', '(', ')']\n",
    "exclude = set(exclude)\n",
    "eng1 = ''.join(ch for ch in eng if ch not in exclude)\n",
    "eng_tag = '''I PRON\n",
    "am AUX\n",
    "going VERB\n",
    "to PREP\n",
    "protest NOUN\n",
    "for PREP\n",
    "a DET\n",
    "right NOUN\n",
    "to PREP\n",
    "name NOUN\n",
    "this DET\n",
    "place NOUN\n",
    "whatever PRON\n",
    "I PRON\n",
    "want VERB\n",
    "You PRON\n",
    "need VERB\n",
    "to PREP\n",
    "rate VERB\n",
    "every DET\n",
    "rain NOUN\n",
    "you PRON\n",
    "had VERB\n",
    "in PREP\n",
    "your PRON\n",
    "life NOUN\n",
    "Give VERB\n",
    "me PRON\n",
    "a DET\n",
    "sign NOUN\n",
    "if CONJ\n",
    "you PRON\n",
    "want VERB\n",
    "to PART\n",
    "smoke VERB\n",
    "some DET\n",
    "stuff NOUN\n",
    "with PREP\n",
    "me PRON\n",
    "She PRON\n",
    "is VERB\n",
    "a DET\n",
    "real ADJ\n",
    "tease NOUN\n",
    "Use VERB\n",
    "some DET\n",
    "time NOUN\n",
    "to PART\n",
    "vote VERB\n",
    "Wake VERB\n",
    "up PREP\n",
    "and CONJ\n",
    "help VERB\n",
    "me PRON\n",
    "find VERB\n",
    "my PRON\n",
    "watch NOUN\n",
    "He PRON\n",
    "gave VERB\n",
    "me PRON\n",
    "a DET\n",
    "wave NOUN\n",
    "and CONJ\n",
    "a DET\n",
    "wink NOUN\n",
    "His PRON\n",
    "voice NOUN\n",
    "is VERB\n",
    "so ADV\n",
    "calm ADJ\n",
    "it PRON\n",
    "makes VERB\n",
    "me PRON\n",
    "yawn VERB\n",
    "I PRON\n",
    "heard VERB\n",
    "his PRON\n",
    "voice NOUN\n",
    "and CONJ\n",
    "looked VERB\n",
    "upstage ADJ\n",
    "I PRON\n",
    "want VERB\n",
    "to PART\n",
    "take VERB\n",
    "a DET\n",
    "risk NOUN\n",
    "and CONJ\n",
    "participate VERB\n",
    "in PREP\n",
    "the CONJ\n",
    "race NOUN\n",
    "for PREP\n",
    "a DET\n",
    "gold ADJ\n",
    "flower NOUN\n",
    "Did AUX\n",
    "you PRON\n",
    "burn VERB\n",
    "the DET\n",
    "cake NOUN'''\n",
    "pos_eng = []\n",
    "for i in eng_tag.split('\\n'):\n",
    "    pos_eng.append(i.split()[1])\n",
    "gold_eng = pos_eng.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем предложения с помощью Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(eng1)\n",
    "spacy_tags = []\n",
    "for i, s in enumerate(doc.sents):\n",
    "    for t in s:\n",
    "        spacy_tags.append(t.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "print(len(spacy_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем предложения с помощью Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-19 23:25:08,052 loading file /Users/apple/.flair/models/en-pos-ontonotes-v0.5.pt\n",
      "2020-10-19 23:25:13,493 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "tagger = SequenceTagger.load('pos')\n",
    "t = []\n",
    "for sentence in re.split(r'\\?|!|\\.', eng):\n",
    "    sentence1 = Sentence(sentence)\n",
    "    tagger.predict(sentence1)\n",
    "    tags = sentence1.to_tagged_string()\n",
    "    tags = tags.split()[1::2]\n",
    "    for i in tags:\n",
    "        t.append(re.sub(r'>|<', '', i))\n",
    "    tags_flair = t\n",
    "print(len(tags_flair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем предложения с помощью NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "sents = re.split(r'\\?|!|\\.', eng)\n",
    "[word_tokenize(sent) for sent in sents]\n",
    "toks = []\n",
    "for sent in sents:\n",
    "    toks.append(pos_tag(word_tokenize(sent)))\n",
    "tags_nltk = []\n",
    "for s in toks:\n",
    "    if s != []:\n",
    "        for w in s:\n",
    "            tags_nltk.append(w[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем все к формату золотого стандарта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_to_gold(pos, type_p):\n",
    "    for i in range(len(pos)):\n",
    "        if type_p == 'nltk':\n",
    "            if pos[i] == 'VBZ' or pos[i] == 'VB' or pos[i] == 'VBP' or pos[i] == 'VBD' or pos[i] == 'VBG':\n",
    "                pos[i] = 'VERB'\n",
    "            if pos[i] == 'NNP' or pos[i] == 'NN':\n",
    "                pos[i] = 'NOUN'\n",
    "            if pos[i] == 'WDT' or pos[i] == 'DT':\n",
    "                pos[i] = 'DET'\n",
    "            if pos[i] == 'PRP' or pos[i] == 'PRP$':\n",
    "                pos[i] = 'PRON'\n",
    "            if pos[i] == 'RP' or pos[i] == 'TO':\n",
    "                pos[i] = 'PART'\n",
    "            if pos[i] == 'JJ':\n",
    "                pos[i] = 'ADJ'\n",
    "            if pos[i] == 'RB':\n",
    "                pos[i] = 'ADV'\n",
    "            if pos[i] == 'CC':\n",
    "                pos[i] = 'CONJ'\n",
    "            if pos[i] == 'IN':\n",
    "                pos[i] = 'PREP'\n",
    "        if type_p == 'flair':\n",
    "            if pos[i] == 'DT':\n",
    "                pos[i] = 'DET'\n",
    "            if pos[i] == 'VBZ' or pos[i] == 'VB' or pos[i] == 'VBP' or pos[i] == 'VBD' or pos[i] == 'VBG':\n",
    "                pos[i] = 'VERB'\n",
    "            if pos[i] == 'NNP' or pos[i] == 'NN':\n",
    "                pos[i] = 'NOUN'\n",
    "            if pos[i] == 'WDT' or pos[i] == 'DT':\n",
    "                pos[i] = 'DET'\n",
    "            if pos[i] == 'PRP' or pos[i] == 'PRP$':\n",
    "                pos[i] = 'PRON'\n",
    "            if pos[i] == 'RP' or pos[i] == 'TO':\n",
    "                pos[i] = 'PART'\n",
    "            if pos[i] == 'JJ':\n",
    "                pos[i] = 'ADJ'\n",
    "            if pos[i] == 'RB':\n",
    "                pos[i] = 'ADV'\n",
    "            if pos[i] == 'CC':\n",
    "                pos[i] = 'CONJ'\n",
    "            if pos[i] == 'IN':\n",
    "                pos[i] = 'PREP'\n",
    "        if type_p == 'spacy':\n",
    "            if pos[i] == 'PROPN':\n",
    "                pos[i] = 'NOUN'\n",
    "            if pos[i] == 'CCONJ' or pos[i] == 'SCONJ':\n",
    "                pos[i] = 'CONJ'\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_gold = other_to_gold(spacy_tags, 'spacy')\n",
    "nltk_gold = other_to_gold(tags_nltk, 'nltk')\n",
    "flair_gold = other_to_gold(tags_flair, 'nltk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем accuracy каждого таггера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy's accuracy: 0.7623762376237624\n",
      "nltk's accuracy: 0.8613861386138614\n",
      "flair's accuracy: 0.8811881188118812\n"
     ]
    }
   ],
   "source": [
    "print(\"spacy's accuracy:\", get_acc(spacy_gold, gold_eng))\n",
    "print(\"nltk's accuracy:\", get_acc(nltk_gold, gold_eng))\n",
    "print(\"flair's accuracy:\", get_acc(flair_gold, gold_eng))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
