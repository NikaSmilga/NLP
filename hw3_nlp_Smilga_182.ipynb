{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже будет предобработка текста и прогон генсима. По сути я просто взяла это из семинара по генсиму :) Еще я буду использовать модуль TQDM для измерения времени, тк в этой домашке все работает жутко долго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.content.values.tolist()\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in tqdm(texts):\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11314/11314 [07:02<00:00, 26.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 'thing', 'car', 'nntp_poste', 'host', 'park', 'line', 'wonder', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут добавляем столбец с лемматизированными текстами в наш ДатаФрейм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_lemmatized'] = data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('addition', 1),\n",
       "  ('body', 1),\n",
       "  ('bring', 1),\n",
       "  ('call', 1),\n",
       "  ('car', 5),\n",
       "  ('could', 1),\n",
       "  ('day', 1),\n",
       "  ('door', 2),\n",
       "  ('early', 1),\n",
       "  ('engine', 1),\n",
       "  ('enlighten', 1),\n",
       "  ('funky', 1),\n",
       "  ('history', 1),\n",
       "  ('host', 1),\n",
       "  ('info', 1),\n",
       "  ('know', 1),\n",
       "  ('late', 1),\n",
       "  ('lerxst', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 2),\n",
       "  ('mail', 1),\n",
       "  ('make', 1),\n",
       "  ('model', 1),\n",
       "  ('name', 1),\n",
       "  ('neighborhood', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('park', 1),\n",
       "  ('production', 1),\n",
       "  ('really', 1),\n",
       "  ('rest', 1),\n",
       "  ('see', 1),\n",
       "  ('separate', 1),\n",
       "  ('small', 1),\n",
       "  ('spec', 1),\n",
       "  ('sport', 1),\n",
       "  ('thank', 1),\n",
       "  ('thing', 1),\n",
       "  ('where', 1),\n",
       "  ('wonder', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем модель с рандомным кол-вом топиков просто чтобы посмотреть, верно ли мы все сделали ранее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель обучилась, все хорошо. Проверяем топики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.115*\"player\" + 0.103*\"disk\" + 0.058*\"format\" + 0.054*\"network\" + '\n",
      "  '0.044*\"data\" + 0.033*\"electronic\" + 0.030*\"factory\" + 0.029*\"rsa\" + '\n",
      "  '0.024*\"datum\" + 0.019*\"regardless\"'),\n",
      " (1,\n",
      "  '0.030*\"evidence\" + 0.027*\"believe\" + 0.027*\"say\" + 0.026*\"reason\" + '\n",
      "  '0.020*\"claim\" + 0.018*\"people\" + 0.017*\"sense\" + 0.015*\"exist\" + '\n",
      "  '0.014*\"may\" + 0.014*\"faith\"'),\n",
      " (2,\n",
      "  '0.082*\"space\" + 0.049*\"image\" + 0.029*\"com\" + 0.028*\"orbit\" + '\n",
      "  '0.023*\"launch\" + 0.019*\"mission\" + 0.019*\"earth\" + 0.018*\"plane\" + '\n",
      "  '0.017*\"surface\" + 0.015*\"shuttle\"'),\n",
      " (3,\n",
      "  '0.157*\"car\" + 0.053*\"bike\" + 0.044*\"engine\" + 0.040*\"ride\" + 0.034*\"ground\" '\n",
      "  '+ 0.027*\"rider\" + 0.027*\"vehicle\" + 0.024*\"gas\" + 0.024*\"door\" + '\n",
      "  '0.022*\"mile\"'),\n",
      " (4,\n",
      "  '0.701*\"ax\" + 0.020*\"monitor\" + 0.019*\"character\" + 0.017*\"master\" + '\n",
      "  '0.017*\"normal\" + 0.013*\"sorry\" + 0.010*\"font\" + 0.009*\"excuse\" + '\n",
      "  '0.009*\"benefit\" + 0.009*\"sc\"'),\n",
      " (5,\n",
      "  '0.097*\"law\" + 0.089*\"encryption\" + 0.052*\"security\" + 0.051*\"clipper\" + '\n",
      "  '0.048*\"public\" + 0.045*\"tap\" + 0.045*\"enforcement\" + 0.037*\"government\" + '\n",
      "  '0.026*\"privacy\" + 0.021*\"court\"'),\n",
      " (6,\n",
      "  '0.044*\"church\" + 0.043*\"village\" + 0.036*\"turkish\" + 0.031*\"soldier\" + '\n",
      "  '0.030*\"occupy\" + 0.025*\"jewish\" + 0.022*\"terrorist\" + 0.021*\"woman\" + '\n",
      "  '0.020*\"withdraw\" + 0.019*\"armenian\"'),\n",
      " (7,\n",
      "  '0.081*\"gun\" + 0.052*\"israeli\" + 0.051*\"kill\" + 0.036*\"weapon\" + '\n",
      "  '0.032*\"military\" + 0.031*\"crime\" + 0.029*\"war\" + 0.025*\"police\" + '\n",
      "  '0.023*\"attack\" + 0.020*\"death\"'),\n",
      " (8,\n",
      "  '0.080*\"value\" + 0.072*\"error\" + 0.068*\"instal\" + 0.065*\"function\" + '\n",
      "  '0.041*\"object\" + 0.036*\"cpu\" + 0.035*\"library\" + 0.033*\"motorcycle\" + '\n",
      "  '0.033*\"boy\" + 0.028*\"ram\"'),\n",
      " (9,\n",
      "  '0.196*\"key\" + 0.126*\"chip\" + 0.076*\"phone\" + 0.054*\"tape\" + '\n",
      "  '0.048*\"technology\" + 0.031*\"bit\" + 0.031*\"secure\" + 0.030*\"direct\" + '\n",
      "  '0.024*\"secret\" + 0.023*\"block\"'),\n",
      " (10,\n",
      "  '0.024*\"people\" + 0.023*\"state\" + 0.020*\"right\" + 0.018*\"issue\" + '\n",
      "  '0.017*\"government\" + 0.015*\"case\" + 0.012*\"group\" + 0.012*\"public\" + '\n",
      "  '0.011*\"report\" + 0.011*\"law\"'),\n",
      " (11,\n",
      "  '0.108*\"team\" + 0.091*\"game\" + 0.083*\"play\" + 0.058*\"win\" + 0.044*\"year\" + '\n",
      "  '0.036*\"internet\" + 0.035*\"run\" + 0.033*\"score\" + 0.028*\"goal\" + '\n",
      "  '0.022*\"owner\"'),\n",
      " (12,\n",
      "  '0.030*\"use\" + 0.023*\"system\" + 0.020*\"thank\" + 0.019*\"line\" + 0.017*\"file\" '\n",
      "  '+ 0.017*\"program\" + 0.014*\"need\" + 0.014*\"mail\" + 0.013*\"include\" + '\n",
      "  '0.013*\"window\"'),\n",
      " (13,\n",
      "  '0.022*\"write\" + 0.022*\"would\" + 0.018*\"line\" + 0.013*\"know\" + 0.013*\"go\" + '\n",
      "  '0.012*\"be\" + 0.012*\"make\" + 0.011*\"get\" + 0.011*\"post\" + 0.011*\"think\"'),\n",
      " (14,\n",
      "  '0.086*\"power\" + 0.082*\"light\" + 0.036*\"cool\" + 0.033*\"hot\" + 0.031*\"cable\" '\n",
      "  '+ 0.030*\"upgrade\" + 0.030*\"industry\" + 0.029*\"cycle\" + 0.026*\"nuclear\" + '\n",
      "  '0.025*\"prefer\"'),\n",
      " (15,\n",
      "  '0.085*\"science\" + 0.083*\"black\" + 0.052*\"univ\" + 0.043*\"switch\" + '\n",
      "  '0.043*\"office\" + 0.038*\"manual\" + 0.034*\"hole\" + 0.033*\"blind\" + '\n",
      "  '0.032*\"specify\" + 0.028*\"oil\"'),\n",
      " (16,\n",
      "  '0.058*\"human\" + 0.057*\"die\" + 0.046*\"death\" + 0.027*\"authority\" + '\n",
      "  '0.024*\"trial\" + 0.021*\"generally\" + 0.019*\"step\" + 0.019*\"greek\" + '\n",
      "  '0.017*\"kill\" + 0.016*\"prepare\"'),\n",
      " (17,\n",
      "  '0.081*\"entry\" + 0.062*\"page\" + 0.046*\"publish\" + 0.042*\"number\" + '\n",
      "  '0.031*\"average\" + 0.028*\"contain\" + 0.028*\"sun\" + 0.027*\"official\" + '\n",
      "  '0.026*\"baseball\" + 0.026*\"homosexual\"'),\n",
      " (18,\n",
      "  '0.130*\"drive\" + 0.079*\"card\" + 0.052*\"driver\" + 0.036*\"fast\" + '\n",
      "  '0.035*\"color\" + 0.033*\"speed\" + 0.029*\"device\" + 0.028*\"bus\" + 0.025*\"slow\" '\n",
      "  '+ 0.024*\"rate\"'),\n",
      " (19,\n",
      "  '0.129*\"memory\" + 0.102*\"video\" + 0.098*\"screen\" + 0.063*\"mouse\" + '\n",
      "  '0.044*\"board\" + 0.040*\"period\" + 0.034*\"gateway\" + 0.026*\"bank\" + '\n",
      "  '0.022*\"icon\" + 0.019*\"expand\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Маллет у меня отказался работать. Джава устанавливаться не хочет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mallet_path = '/Users/apple/Downloads/mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=i, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем функцию, выбирающую наилучшее количество топиков для имеющихся у нас данных. Я ставлю значения от 5 до 36 и шаг 5, иначе программа будет работать невозможно долго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_the_best(text, dicti, corpus):\n",
    "    b = 0\n",
    "    for i in tqdm(range(5,36,5)):\n",
    "        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dicti,\n",
    "                                           num_topics=i, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=text, dictionary=dicti, coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        print(coherence_lda)\n",
    "        if coherence_lda > b:\n",
    "            b = coherence_lda\n",
    "            best = i\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоняем функцию и смотрим лучший вариант. Я распечатываю coherence каждый раз, чтоб убедиться, что функция работает корректно и действительно выбирает наилучший вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [01:25<08:35, 85.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5269441951105925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|██▊       | 2/7 [08:30<15:37, 187.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5466767470131106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|████▎     | 3/7 [12:59<14:07, 211.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.511221078285004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|█████▋    | 4/7 [16:15<10:21, 207.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5294120587674489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|███████▏  | 5/7 [19:54<07:01, 210.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45480934007509133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|████████▌ | 6/7 [24:16<03:46, 226.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5036156445576092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [29:04<00:00, 249.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.474271454959223\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best = choose_the_best(data_lemmatized, id2word, corpus)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили наилучший вариант 10. Далее работаем с ним. Сначала обучаем модель с нужным нам количеством топиков best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=best, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На всякий случай смотрим топики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  [('file', 0.027850587),\n",
       "   ('window', 0.021941382),\n",
       "   ('program', 0.02191324),\n",
       "   ('use', 0.015722554),\n",
       "   ('problem', 0.014346607),\n",
       "   ('set', 0.013252458),\n",
       "   ('run', 0.0122761605),\n",
       "   ('entry', 0.011657789),\n",
       "   ('image', 0.011535945),\n",
       "   ('memory', 0.01123172)]),\n",
       " (1,\n",
       "  [('people', 0.01993293),\n",
       "   ('would', 0.015832035),\n",
       "   ('say', 0.015714714),\n",
       "   ('believe', 0.011121675),\n",
       "   ('think', 0.011040432),\n",
       "   ('reason', 0.011005266),\n",
       "   ('make', 0.009948168),\n",
       "   ('evidence', 0.009848934),\n",
       "   ('may', 0.009772895),\n",
       "   ('write', 0.008406777)]),\n",
       " (2,\n",
       "  [('people', 0.0136572225),\n",
       "   ('say', 0.0127685545),\n",
       "   ('kill', 0.011285022),\n",
       "   ('child', 0.010960716),\n",
       "   ('government', 0.008870921),\n",
       "   ('death', 0.0077233817),\n",
       "   ('attack', 0.0073048105),\n",
       "   ('day', 0.006969694),\n",
       "   ('man', 0.0062017706),\n",
       "   ('country', 0.0061769187)]),\n",
       " (3,\n",
       "  [('line', 0.034186095),\n",
       "   ('write', 0.02520564),\n",
       "   ('would', 0.018932294),\n",
       "   ('be', 0.016619122),\n",
       "   ('go', 0.015758323),\n",
       "   ('get', 0.0149134975),\n",
       "   ('article', 0.014830229),\n",
       "   ('know', 0.013797913),\n",
       "   ('good', 0.011888776),\n",
       "   ('host', 0.011536149)]),\n",
       " (4,\n",
       "  [('internet', 0.05190312),\n",
       "   ('print', 0.035394114),\n",
       "   ('printer', 0.023439571),\n",
       "   ('keyboard', 0.019489588),\n",
       "   ('translation', 0.018814914),\n",
       "   ('font', 0.017733593),\n",
       "   ('multiple', 0.016261326),\n",
       "   ('slightly', 0.014130624),\n",
       "   ('localtalk', 0.013612571),\n",
       "   ('agent', 0.013117739)]),\n",
       " (5,\n",
       "  [('space', 0.053285882),\n",
       "   ('sphere', 0.016994765),\n",
       "   ('orbit', 0.015541011),\n",
       "   ('launch', 0.01536247),\n",
       "   ('mission', 0.014469475),\n",
       "   ('food', 0.012781231),\n",
       "   ('earth', 0.011959183),\n",
       "   ('satellite', 0.011357369),\n",
       "   ('plane', 0.011316957),\n",
       "   ('test', 0.01070201)]),\n",
       " (6,\n",
       "  [('system', 0.024963329),\n",
       "   ('key', 0.016170064),\n",
       "   ('use', 0.015161433),\n",
       "   ('mail', 0.013088656),\n",
       "   ('information', 0.011897514),\n",
       "   ('send', 0.010684329),\n",
       "   ('card', 0.01034338),\n",
       "   ('need', 0.009571339),\n",
       "   ('include', 0.009462866),\n",
       "   ('computer', 0.009235664)]),\n",
       " (7,\n",
       "  [('drive', 0.18766624),\n",
       "   ('driver', 0.08068204),\n",
       "   ('slow', 0.03768438),\n",
       "   ('speed', 0.036161102),\n",
       "   ('fast', 0.027976949),\n",
       "   ('scsi', 0.027411425),\n",
       "   ('busmastere', 0.02392214),\n",
       "   ('install', 0.021524372),\n",
       "   ('ide', 0.018588457),\n",
       "   ('performance', 0.0116264485)]),\n",
       " (8,\n",
       "  [('ax', 0.52775234),\n",
       "   ('season', 0.018432513),\n",
       "   ('hockey', 0.018082788),\n",
       "   ('playoff', 0.010179792),\n",
       "   ('capable', 0.00986655),\n",
       "   ('pen', 0.0092050005),\n",
       "   ('evil', 0.008370501),\n",
       "   ('ball', 0.0062697455),\n",
       "   ('slip', 0.006117748),\n",
       "   ('coach', 0.0058940086)]),\n",
       " (9,\n",
       "  [('case', 0.007843288),\n",
       "   ('may', 0.007831903),\n",
       "   ('book', 0.0074574095),\n",
       "   ('also', 0.0064877323),\n",
       "   ('number', 0.0062460112),\n",
       "   ('report', 0.005645848),\n",
       "   ('group', 0.0055968794),\n",
       "   ('year', 0.005448726),\n",
       "   ('result', 0.005242662),\n",
       "   ('science', 0.005124538)])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.show_topics(formatted=False, num_topics=best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словарь словарей, где каждому ключу-топику соответствует значение-словарь, в котором ключ -- слово, а значение -- его вес в этом топике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dict = {}\n",
    "for i in lda_model.show_topics(formatted=False, num_topics=31):\n",
    "    word_weight = {}\n",
    "    for word in i[1]:\n",
    "        word_weight[word[0]] = word[1]\n",
    "    topic_dict[i[0]] = word_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем функцию, достающую топики из текста. Если ни одного слова не встретилось, присваиваем топик unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic(text):\n",
    "    dict_prob = {}\n",
    "    for word in text:\n",
    "        for i in range(len(topic_dict)):\n",
    "            if word in topic_dict[i]:\n",
    "                if i in dict_prob:\n",
    "                    dict_prob[i] += topic_dict[i][word]\n",
    "                else:\n",
    "                    dict_prob[i] = topic_dict[i][word]\n",
    "    if dict_prob != {}:\n",
    "        topic = sorted(dict_prob.items(), key=lambda kv: kv[1], reverse=True)[0][0]\n",
    "    else:\n",
    "        topic = 'unknown'\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, работает ли она на коротком простом тексте(он содержит два слова из топика 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(get_topic(['program', 'run']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применяем ее к целому столбцу нашего датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topic'] = df['data_lemmatized'].apply(get_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, корректно ли все получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content  target  \\\n",
      "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
      "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
      "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
      "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
      "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
      "...                                                  ...     ...   \n",
      "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...      13   \n",
      "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...       4   \n",
      "11311  From: westes@netcom.com (Will Estes)\\nSubject:...       3   \n",
      "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...       1   \n",
      "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...       8   \n",
      "\n",
      "                   target_names  \\\n",
      "0                     rec.autos   \n",
      "1         comp.sys.mac.hardware   \n",
      "2         comp.sys.mac.hardware   \n",
      "3                 comp.graphics   \n",
      "4                     sci.space   \n",
      "...                         ...   \n",
      "11309                   sci.med   \n",
      "11310     comp.sys.mac.hardware   \n",
      "11311  comp.sys.ibm.pc.hardware   \n",
      "11312             comp.graphics   \n",
      "11313           rec.motorcycles   \n",
      "\n",
      "                                         data_lemmatized topic  \n",
      "0      [where, thing, car, nntp_poste, host, park, li...     3  \n",
      "1      [si, poll, final, summary, final, call, si, cl...     7  \n",
      "2      [question, engineering, computer, network, dis...     3  \n",
      "3      [division, line, host, amber, write, write, ar...     3  \n",
      "4      [question, organization, smithsonian_astrophys...     3  \n",
      "...                                                  ...   ...  \n",
      "11309  [migraine, city, ny_bis, reply, line, cheap, a...     3  \n",
      "11310  [problem, screen, blank, sometimes, minor, phy...     7  \n",
      "11311  [este, mount, case, organization, mail, group,...     4  \n",
      "11312  [line, nntp_poste, host, article, write, boy, ...     5  \n",
      "11313  [gun, steal, organization, line, distribution_...     3  \n",
      "\n",
      "[11314 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее решаем задание с tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня получилось чудовищно много текстов четвертого топика, поэтому на них падает и мой комп, и коллаб. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th>data_lemmatized</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>702</td>\n",
       "      <td>702</td>\n",
       "      <td>702</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8272</td>\n",
       "      <td>8272</td>\n",
       "      <td>8272</td>\n",
       "      <td>8272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1018</td>\n",
       "      <td>1018</td>\n",
       "      <td>1018</td>\n",
       "      <td>1018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unknown</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         content  target  target_names  data_lemmatized\n",
       "topic                                                  \n",
       "0            513     513           513              513\n",
       "1            702     702           702              702\n",
       "2             41      41            41               41\n",
       "3           8272    8272          8272             8272\n",
       "4            163     163           163              163\n",
       "5            154     154           154              154\n",
       "6            372     372           372              372\n",
       "7           1018    1018          1018             1018\n",
       "8             45      45            45               45\n",
       "9             12      12            12               12\n",
       "unknown       22      22            22               22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('topic').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я разделю ДФ на 5 частей, обработаю каждую отдельно, затем соединю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[:2500].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[2500:5000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df[5000:7500].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df[7500:10000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df[10000:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам функцию для вычисления tf-idf, которая будет запускать цикл для каждого топика. К текстам неопознанного топика я осознанно ее не применяю."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_tfidf(df):\n",
    "    for i in tqdm(range(best)):\n",
    "        topic = df.loc[df['topic'] == i, 'data_lemmatized']\n",
    "        topic_text = [' '.join(i) for i in topic]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectors = vectorizer.fit_transform(topic_text)\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        dense = vectors.todense()\n",
    "        denselist = dense.tolist()\n",
    "        df_tfidf = pd.DataFrame(denselist, columns=feature_names).T\n",
    "        df_tfidf.columns = list(df.index[df['topic'] == i])\n",
    "        for col in df_tfidf.columns:\n",
    "            top5 = list(df_tfidf[col].sort_values(ascending=False).head().index)\n",
    "            df.loc[col, 'top_words'] = ' '.join(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применю функцию последовательно к каждому из 5 кусочков исходного датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:01<00:00,  6.17s/it]\n",
      "100%|██████████| 10/10 [00:57<00:00,  5.73s/it]\n",
      "100%|██████████| 10/10 [00:52<00:00,  5.22s/it]\n",
      "100%|██████████| 10/10 [00:55<00:00,  5.58s/it]\n",
      "100%|██████████| 10/10 [00:15<00:00,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "insert_tfidf(df1)\n",
    "insert_tfidf(df2)\n",
    "insert_tfidf(df3)\n",
    "insert_tfidf(df4)\n",
    "insert_tfidf(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Склею получившиеся датафреймы в один."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df1, df2, df3, df4, df5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Распечатаю получившийся датафрейм, чтобы показать, что все получилось правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 content  target  \\\n",
      "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
      "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
      "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
      "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
      "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
      "...                                                  ...     ...   \n",
      "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...      13   \n",
      "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...       4   \n",
      "11311  From: westes@netcom.com (Will Estes)\\nSubject:...       3   \n",
      "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...       1   \n",
      "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...       8   \n",
      "\n",
      "                   target_names  \\\n",
      "0                     rec.autos   \n",
      "1         comp.sys.mac.hardware   \n",
      "2         comp.sys.mac.hardware   \n",
      "3                 comp.graphics   \n",
      "4                     sci.space   \n",
      "...                         ...   \n",
      "11309                   sci.med   \n",
      "11310     comp.sys.mac.hardware   \n",
      "11311  comp.sys.ibm.pc.hardware   \n",
      "11312             comp.graphics   \n",
      "11313           rec.motorcycles   \n",
      "\n",
      "                                         data_lemmatized topic  \\\n",
      "0      [where, thing, car, nntp_poste, host, park, li...     3   \n",
      "1      [si, poll, final, summary, final, call, si, cl...     7   \n",
      "2      [question, engineering, computer, network, dis...     3   \n",
      "3      [division, line, host, amber, write, write, ar...     3   \n",
      "4      [question, organization, smithsonian_astrophys...     3   \n",
      "...                                                  ...   ...   \n",
      "11309  [migraine, city, ny_bis, reply, line, cheap, a...     3   \n",
      "11310  [problem, screen, blank, sometimes, minor, phy...     7   \n",
      "11311  [este, mount, case, organization, mail, group,...     4   \n",
      "11312  [line, nntp_poste, host, article, write, boy, ...     5   \n",
      "11313  [gun, steal, organization, line, distribution_...     3   \n",
      "\n",
      "                                      top_words  \n",
      "0                   car door lerxst funky where  \n",
      "1                   clock poll si final upgrade  \n",
      "2               display machine bunch hear disk  \n",
      "3      division chip quadrilateral weitek amber  \n",
      "4                  error bug warn memory expect  \n",
      "...                                         ...  \n",
      "11309                patient eeg dn tumor brain  \n",
      "11310      blank wire screen sometimes computer  \n",
      "11311              cpu mount cooler este weight  \n",
      "11312                point sphere see boy mercy  \n",
      "11313               steal xpm kjg willow spring  \n",
      "\n",
      "[11314 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберу ненужные в итоге столбцы target и target_names (в задании нигде не сказано, что они требуются в финальном df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del result['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del result['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Покажу вам самый финальный датафрейм."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>data_lemmatized</th>\n",
       "      <th>topic</th>\n",
       "      <th>top_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>[where, thing, car, nntp_poste, host, park, li...</td>\n",
       "      <td>3</td>\n",
       "      <td>car door lerxst funky where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>[si, poll, final, summary, final, call, si, cl...</td>\n",
       "      <td>7</td>\n",
       "      <td>clock poll si final upgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>[question, engineering, computer, network, dis...</td>\n",
       "      <td>3</td>\n",
       "      <td>display machine bunch hear disk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>[division, line, host, amber, write, write, ar...</td>\n",
       "      <td>3</td>\n",
       "      <td>division chip quadrilateral weitek amber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>[question, organization, smithsonian_astrophys...</td>\n",
       "      <td>3</td>\n",
       "      <td>error bug warn memory expect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "      <td>[migraine, city, ny_bis, reply, line, cheap, a...</td>\n",
       "      <td>3</td>\n",
       "      <td>patient eeg dn tumor brain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n",
       "      <td>[problem, screen, blank, sometimes, minor, phy...</td>\n",
       "      <td>7</td>\n",
       "      <td>blank wire screen sometimes computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "      <td>[este, mount, case, organization, mail, group,...</td>\n",
       "      <td>4</td>\n",
       "      <td>cpu mount cooler este weight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "      <td>[line, nntp_poste, host, article, write, boy, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>point sphere see boy mercy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n",
       "      <td>[gun, steal, organization, line, distribution_...</td>\n",
       "      <td>3</td>\n",
       "      <td>steal xpm kjg willow spring</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  \\\n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...   \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...   \n",
       "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...   \n",
       "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...   \n",
       "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...   \n",
       "...                                                  ...   \n",
       "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...   \n",
       "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...   \n",
       "11311  From: westes@netcom.com (Will Estes)\\nSubject:...   \n",
       "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...   \n",
       "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...   \n",
       "\n",
       "                                         data_lemmatized topic  \\\n",
       "0      [where, thing, car, nntp_poste, host, park, li...     3   \n",
       "1      [si, poll, final, summary, final, call, si, cl...     7   \n",
       "2      [question, engineering, computer, network, dis...     3   \n",
       "3      [division, line, host, amber, write, write, ar...     3   \n",
       "4      [question, organization, smithsonian_astrophys...     3   \n",
       "...                                                  ...   ...   \n",
       "11309  [migraine, city, ny_bis, reply, line, cheap, a...     3   \n",
       "11310  [problem, screen, blank, sometimes, minor, phy...     7   \n",
       "11311  [este, mount, case, organization, mail, group,...     4   \n",
       "11312  [line, nntp_poste, host, article, write, boy, ...     5   \n",
       "11313  [gun, steal, organization, line, distribution_...     3   \n",
       "\n",
       "                                      top_words  \n",
       "0                   car door lerxst funky where  \n",
       "1                   clock poll si final upgrade  \n",
       "2               display machine bunch hear disk  \n",
       "3      division chip quadrilateral weitek amber  \n",
       "4                  error bug warn memory expect  \n",
       "...                                         ...  \n",
       "11309                patient eeg dn tumor brain  \n",
       "11310      blank wire screen sometimes computer  \n",
       "11311              cpu mount cooler este weight  \n",
       "11312                point sphere see boy mercy  \n",
       "11313               steal xpm kjg willow spring  \n",
       "\n",
       "[11314 rows x 4 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По сути coherence (связность) score показывает, насколько получившийся топик хорош в терминах интерпретируемости человеком (то есть насколько он вообще имеет смысл). Эта мера помогает нам не принимать во внимание топики, которые получились из-за простого статистического совпадения и не имеют практического смысла для решаемой задачи (непонятны человеку). Coherence score оценивается через измерение семантического сходства между наиболее важными для топика словами, то есть словами, получившими наибольший вес в процессе выделения топика. То есть мы смотрим, насколько часто наиболее важные для модели слова встречаются в текстах корпуса вместе, насколько они вообще в целом имеют связь между собой."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
